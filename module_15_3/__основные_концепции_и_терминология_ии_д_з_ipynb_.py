# -*- coding: utf-8 -*-
""""_Основные концепции и терминология ИИ. Д.З.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nq3662CroZsr6x1shzgLwRAZvMyNgwqk

# Домашнее задание

Ознакомьтесь с приведёнными ниже примерами использования алгоритмов МО и НС для решения задачи распознавания рукописных цифр.

## Порядок выполнения ДЗ

1. Сделайте копию данного блокнота себе на диск. Далее работайте со своей копией блокнота. Сохраняйте вносимые в неё изменения.
2. Ознакомьтесь с теоретическим текстом и кодом из настоящего блокнота.
3. Перенесите примеры кода в отдельные кодовые ячейки и выполните их.
4. Создайте тестовую ячейку, куда запишите ответы на теоретические вопросы.
5. Расшарьте блокнот и используйте ссылку как ответ на ДЗ.

### Учебный пример: Рещение задачи классификация рукописных цифр с использованием машинного обучения, глубокого обучения и нейронных сетей

В этом задании мы будем использовать набор данных MNIST, который содержит изображения рукописных цифр (от 0 до 9). Мы реализуем три различных подхода к классификации этих изображений:

1. **Машинное обучение**: Используем метод k-ближайших соседей (k-NN).
2. **Глубокое обучение**: Используем многослойный перцептрон (MLP).
3. **Нейронные сети**: Используем сверточную нейронную сеть (CNN).

### Шаг 1: Установка библиотек

Установите необходимые библиотеки:

```bash
pip install numpy pandas scikit-learn tensorflow keras
```

### Шаг 2: Загрузка и предобработка данных

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical

# Загрузка данных MNIST
mnist = fetch_openml('mnist_784', version=1)
X, y = mnist.data / 255.0, mnist.target.astype(int)

# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### Шаг 3: Алгоритм машинного обучения (k-NN)

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Создание и обучение модели k-NN
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Прогнозирование на тестовой выборке
y_pred_knn = knn.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred_knn)

print(f'Accuracy of k-NN: {accuracy_knn:.4f}')
```

### Шаг 4: Глубокое обучение (MLP)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Предобработка меток для MLP
y_train_mlp = to_categorical(y_train, 10)
y_test_mlp = to_categorical(y_test, 10)

# Создание модели MLP
model_mlp = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Компиляция и обучение модели MLP
model_mlp.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_mlp.fit(X_train, y_train_mlp, epochs=10, batch_size=32, validation_split=0.2)

# Оценка модели на тестовой выборке
loss_mlp, accuracy_mlp = model_mlp.evaluate(X_test, y_test_mlp)

print(f'Accuracy of MLP: {accuracy_mlp:.4f}')
```

### Шаг 5: Нейронные сети (CNN)

```python
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout

# Предобработка данных для CNN
X_train_cnn = X_train.values.reshape(-1, 28, 28, 1)
X_test_cnn = X_test.values.reshape(-1, 28, 28, 1)
y_train_cnn = to_categorical(y_train, 10)
y_test_cnn = to_categorical(y_test, 10)

# Создание модели CNN
model_cnn = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

# Компиляция и обучение модели CNN
model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_cnn.fit(X_train_cnn, y_train_cnn, epochs=10, batch_size=32, validation_split=0.2)

# Оценка модели на тестовой выборке
loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test_cnn, y_test_cnn)

print(f'Accuracy of CNN: {accuracy_cnn:.4f}')
```

### Заключение

В этом задании мы реализовали три различных подхода к классификации изображений рукописных цифр с использованием средств машинного обучения (k-NN), глубокого обучения (MLP) и нейронных сетей (CNN). Мы увидели, что каждый из этих подходов имеет свои преимущества и недостатки, и что сложные модели глубокого обучения могут значительно улучшить точность классификации по сравнению с простыми моделями машинного обучения.

### Теоритические вопросы

1. Какие преимущества и недостатки использованных методов вы увидели?
2. В чем, на ваш взгляд, заключается принципиальная разница между многослойным перцептроном и сверточной нейронной сетью?
3. Какие методы предобработки данных были использованы в этом задании?
"""

#Создайте необходимое количество кодовых ячеек и исполните в них приведенный выше код

"""Шаг 1: Установка библиотек

Установите необходимые библиотеки:


"""

pip install numpy pandas scikit-learn tensorflow keras

"""Шаг 2: Загрузка и предобработка данных"""

from google.colab import drive
drive.mount('/content/drive')
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical

# Загрузка данных MNIST
mnist = fetch_openml('mnist_784', version=1)
X, y = mnist.data / 255.0, mnist.target.astype(int)

# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Шаг 3: Алгоритм машинного обучения (k-NN)"""

from google.colab import drive
drive.mount('/content/drive')
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Создание и обучение модели k-NN
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Прогнозирование на тестовой выборке
y_pred_knn = knn.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred_knn)

print(f'Accuracy of k-NN: {accuracy_knn:.4f}')

"""Шаг 4: Глубокое обучение (MLP)"""

from google.colab import drive
drive.mount('/content/drive')
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Предобработка меток для MLP
y_train_mlp = to_categorical(y_train, 10)
y_test_mlp = to_categorical(y_test, 10)

# Создание модели MLP
model_mlp = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Компиляция и обучение модели MLP
model_mlp.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_mlp.fit(X_train, y_train_mlp, epochs=10, batch_size=32, validation_split=0.2)

# Оценка модели на тестовой выборке
loss_mlp, accuracy_mlp = model_mlp.evaluate(X_test, y_test_mlp)

print(f'Accuracy of MLP: {accuracy_mlp:.4f}')

"""Шаг 5: Нейронные сети (CNN)"""

from google.colab import drive
drive.mount('/content/drive')
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout

# Предобработка данных для CNN
X_train_cnn = X_train.values.reshape(-1, 28, 28, 1)
X_test_cnn = X_test.values.reshape(-1, 28, 28, 1)
y_train_cnn = to_categorical(y_train, 10)
y_test_cnn = to_categorical(y_test, 10)

# Создание модели CNN
model_cnn = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

# Компиляция и обучение модели CNN
model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_cnn.fit(X_train_cnn, y_train_cnn, epochs=10, batch_size=32, validation_split=0.2)

# Оценка модели на тестовой выборке
loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test_cnn, y_test_cnn)

print(f'Accuracy of CNN: {accuracy_cnn:.4f}')

"""#Заключение

В этом задании мы реализовали три различных подхода к классификации изображений рукописных цифр с использованием средств машинного обучения (k-NN), глубокого обучения (MLP) и нейронных сетей (CNN). Мы увидели, что каждый из этих подходов имеет свои преимущества и недостатки, и что сложные модели глубокого обучения могут значительно улучшить точность классификации по сравнению с простыми моделями машинного обучения.

#Теоритические вопросы
Какие преимущества и недостатки использованных методов вы увидели?

В чем, на ваш взгляд, заключается принципиальная разница между многослойным перцептроном и сверточной нейронной сетью?

Какие методы предобработки данных были использованы в этом задании?

Преимущества и недостатки методов:

#k-NN (K-ближайших соседей):

Преимущества:

Простота: алгоритм не требует долгой настройки, работает как "ленивый" метод, то есть, запоминает данные и начинает работать только при необходимости классификации.
Эффективен для небольших и средних наборов данных, особенно если есть достаточное количество памяти.

Недостатки:

Требует много памяти, так как сохраняет все данные для классификации.
Медленный на больших выборках, поскольку вычисление расстояний до всех соседей для каждого нового примера занимает много времени.
Чувствителен к масштабированию данных и высокоразмерным данным.

#MLP (Многослойный перцептрон):

Преимущества:

Мощность в решении задач классификации и регрессии за счет нелинейных слоев, которые позволяют MLP решать более сложные задачи по сравнению с k-NN.
Подходит для табличных данных и изображений в плоском формате (например, выровненные вектора пикселей).

Недостатки:

Плохо работает с данными, которые имеют пространственную структуру (например, изображения). Структура MLP не использует пространственную информацию.
Меньшая точность на задачах классификации изображений, чем у специализированных архитектур (например, CNN).

#CNN (Сверточная нейронная сеть):

Преимущества:

Способность выделять пространственные особенности (грани, узоры, объекты) благодаря сверточным слоям, что делает CNN особенно эффективной для задач компьютерного зрения.
Выше точность на задачах классификации изображений, так как CNN автоматически выделяет важные признаки из изображений.

Недостатки:

Более сложная архитектура, требующая настройки и значительных вычислительных ресурсов.
Долгое обучение на больших выборках, требующее мощного оборудования.

#Принципиальная разница между многослойным перцептроном (MLP) и сверточной нейронной сетью (CNN):

1. Многослойный перцептрон (MLP) — это тип нейронной сети с несколькими полносвязными слоями, где каждый нейрон соединен с каждым нейроном следующего слоя. MLP подходит для обработки данных, представленных в плоской (векторной) форме. Он не учитывает пространственные взаимосвязи между признаками, что ограничивает его применение на задачах, где такие взаимосвязи важны, например, в обработке изображений.

2. Сверточная нейронная сеть (CNN), напротив, содержит сверточные слои, которые способны захватывать пространственные структуры данных, такие как пиксели изображения, близкие по положению. CNN выделяет ключевые признаки изображения с помощью сверточных фильтров и пулинговых слоев, позволяя модели быть инвариантной к смещениям и искажениям, что делает CNN идеальной для обработки изображений.

#Методы предобработки данных, использованные в этом задании:

Нормализация данных: данные изображений были нормализованы, делением на 255, чтобы привести их значения к диапазону от 0 до 1. Это позволяет улучшить сходимость модели и стабилизировать обучение.

1. One-Hot Encoding для меток: метки классов были преобразованы с помощью функции to_categorical, чтобы представить их как вектора one-hot для обучения MLP и CNN. Это формат, требуемый для использования с функцией потерь categorical_crossentropy.

2. Реформатирование данных для CNN: изображения были преобразованы в четырехмерные тензоры формы (количество образцов, высота, ширина, каналы), чтобы позволить CNN работать с пространственными свойствами изображения.
"""